{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cedefd2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "import threading\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabdc0b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"Swin Transformer Block for processing facial landmark sequences\"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(dim, window_size, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, int(dim * 4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, dim)\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Window-based multi-head self attention\n",
    "        x = self.attn(x)\n",
    "        x = shortcut + x\n",
    "\n",
    "        # MLP\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = shortcut + x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e13858",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"Window-based Multi-head Self Attention\"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf30a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class SwingTransformerVideoProcessor(nn.Module):\n",
    "    \"\"\"Swing Transformer for processing facial landmark sequences\"\"\"\n",
    "\n",
    "    def __init__(self, landmark_dim=2, embed_dim=128, num_layers=4, num_heads=8, sequence_length=30):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.landmark_dim = landmark_dim\n",
    "\n",
    "        # Embedding layer for landmarks\n",
    "        self.landmark_embed = nn.Linear(landmark_dim, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, sequence_length, embed_dim))\n",
    "\n",
    "        # Swin Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(embed_dim, num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Output heads for different tasks\n",
    "        self.emotion_head = nn.Linear(embed_dim, 7)  # 7 basic emotions\n",
    "        self.attention_head = nn.Linear(embed_dim, 1)  # Attention weights\n",
    "        \n",
    "        # Simple emotion classifier for single frame analysis\n",
    "        self.single_frame_emotion = nn.Sequential(\n",
    "            nn.Linear(landmark_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim // 2, 7)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, landmarks, single_frame_mode=False):\n",
    "        if single_frame_mode:\n",
    "            # Single frame emotion prediction\n",
    "            if len(landmarks.shape) == 3:\n",
    "                # (batch, features) or (batch, seq_len, features) -> use last frame\n",
    "                if landmarks.shape[1] > landmarks.shape[2]:\n",
    "                    landmarks = landmarks[:, -1, :]  # Use last frame\n",
    "                else:\n",
    "                    landmarks = landmarks.mean(dim=1)  # Average if unclear\n",
    "            \n",
    "            emotions = self.single_frame_emotion(landmarks)\n",
    "            # Create dummy attention weights\n",
    "            attention_weights = torch.ones(landmarks.shape[0], landmarks.shape[-1] // 2) * 0.5\n",
    "            return emotions, attention_weights\n",
    "\n",
    "        # Handle different input shapes\n",
    "        if len(landmarks.shape) == 3:\n",
    "            # landmarks shape: (batch_size, sequence_length, features)\n",
    "            B, T, F = landmarks.shape\n",
    "            x = landmarks\n",
    "        elif len(landmarks.shape) == 4:\n",
    "            # landmarks shape: (batch_size, sequence_length, num_landmarks, 2)\n",
    "            B, T, N, D = landmarks.shape\n",
    "            # Flatten landmarks for each timestep\n",
    "            x = landmarks.reshape(B, T, N * D)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected landmark tensor shape: {landmarks.shape}\")\n",
    "\n",
    "        # Embed landmarks\n",
    "        if x.shape[-1] != self.landmark_dim:\n",
    "            # If input features don't match expected dimension, use linear projection\n",
    "            if not hasattr(self, 'input_proj'):\n",
    "                self.input_proj = nn.Linear(x.shape[-1], self.landmark_dim)\n",
    "            x = self.input_proj(x)\n",
    "\n",
    "        x = self.landmark_embed(x)\n",
    "\n",
    "        # Add positional embedding (handle variable sequence lengths)\n",
    "        seq_len = min(T, self.sequence_length)\n",
    "        x = x[:, :seq_len, :] + self.pos_embed[:, :seq_len, :]\n",
    "\n",
    "        # Apply Swin Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Get predictions and attention weights\n",
    "        emotions = self.emotion_head(x.mean(dim=1))  # Global average pooling\n",
    "        attention_weights = torch.softmax(self.attention_head(x).squeeze(-1), dim=1)\n",
    "\n",
    "        return emotions, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d87acd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class FacialLandmarkProcessor:\n",
    "    \"\"\"Process facial landmarks using MediaPipe\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mp_face_mesh = mp.solutions.face_mesh\n",
    "        self.face_mesh = self.mp_face_mesh.FaceMesh(\n",
    "            static_image_mode=False,\n",
    "            max_num_faces=1,\n",
    "            refine_landmarks=True,\n",
    "            min_detection_confidence=0.5,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "\n",
    "        # Key facial landmarks indices (68 key points similar to dlib)\n",
    "        self.key_landmarks = [\n",
    "            # Jaw line (17 points)\n",
    "            10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400,\n",
    "            # Right eyebrow (5 points)\n",
    "            70, 63, 105, 66, 107,\n",
    "            # Left eyebrow (5 points)\n",
    "            296, 334, 293, 300, 276,\n",
    "            # Nose (9 points)\n",
    "            1, 2, 5, 4, 6, 19, 94, 125, 141,\n",
    "            # Right eye (6 points)\n",
    "            33, 7, 163, 144, 145, 153,\n",
    "            # Left eye (6 points)\n",
    "            362, 398, 384, 385, 386, 387,\n",
    "            # Mouth (20 points)\n",
    "            61, 84, 17, 314, 405, 320, 307, 375, 321, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95, 78\n",
    "        ]\n",
    "\n",
    "    def extract_landmarks(self, image):\n",
    "        \"\"\"Extract facial landmarks from image\"\"\"\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = self.face_mesh.process(rgb_image)\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            landmarks = results.multi_face_landmarks[0]\n",
    "            h, w = image.shape[:2]\n",
    "\n",
    "            # Extract key landmarks\n",
    "            key_points = []\n",
    "            for idx in self.key_landmarks:\n",
    "                if idx < len(landmarks.landmark):\n",
    "                    point = landmarks.landmark[idx]\n",
    "                    key_points.append([point.x * w, point.y * h])\n",
    "\n",
    "            return np.array(key_points)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a16820",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class RealTimeVideoProcessor:\n",
    "    \"\"\"Real-time video processing with Swing Transformer\"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length=30):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.landmark_processor = FacialLandmarkProcessor()\n",
    "\n",
    "        # Initialize Swing Transformer\n",
    "        self.model = SwingTransformerVideoProcessor(\n",
    "            landmark_dim=136,  # 68 landmarks * 2 coordinates = 136 features\n",
    "            embed_dim=128,\n",
    "            num_layers=4,\n",
    "            num_heads=8,\n",
    "            sequence_length=sequence_length\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "        # Landmark sequence buffer\n",
    "        self.landmark_buffer = deque(maxlen=sequence_length)\n",
    "\n",
    "        # Emotion labels\n",
    "        self.emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "        # Colors for different facial regions (brighter colors for better visibility)\n",
    "        self.region_colors = {\n",
    "            'jaw': (255, 50, 50),      # Bright Red\n",
    "            'eyebrows': (50, 255, 50),  # Bright Green\n",
    "            'nose': (50, 50, 255),     # Bright Blue\n",
    "            'eyes': (255, 255, 50),    # Bright Yellow\n",
    "            'mouth': (255, 50, 255),   # Bright Magenta\n",
    "        }\n",
    "\n",
    "        # Define regions\n",
    "        self.regions = {\n",
    "            'jaw': list(range(0, 17)),\n",
    "            'eyebrows': list(range(17, 27)),\n",
    "            'nose': list(range(27, 36)),\n",
    "            'eyes': list(range(36, 48)),\n",
    "            'mouth': list(range(48, 68))\n",
    "        }\n",
    "\n",
    "    def normalize_landmarks(self, landmarks):\n",
    "        \"\"\"Normalize landmarks to [-1, 1] range\"\"\"\n",
    "        if landmarks is None:\n",
    "            return None\n",
    "\n",
    "        # Center around mean\n",
    "        center = landmarks.mean(axis=0)\n",
    "        centered = landmarks - center\n",
    "\n",
    "        # Scale to [-1, 1]\n",
    "        scale = np.max(np.abs(centered))\n",
    "        if scale > 0:\n",
    "            normalized = centered / scale\n",
    "        else:\n",
    "            normalized = centered\n",
    "\n",
    "        return normalized\n",
    "\n",
    "    def draw_landmarks_with_features(self, image, landmarks, attention_weights=None, active_features=None):\n",
    "        \"\"\"Draw landmarks with feature highlighting - ENHANCED VERSION\"\"\"\n",
    "        if landmarks is None:\n",
    "            return image\n",
    "\n",
    "        # Create overlay\n",
    "        overlay = image.copy()\n",
    "\n",
    "        # Draw landmarks by region with LARGER sizes\n",
    "        for region_name, indices in self.regions.items():\n",
    "            color = self.region_colors[region_name]\n",
    "\n",
    "            for i, idx in enumerate(indices):\n",
    "                if idx < len(landmarks):\n",
    "                    x, y = int(landmarks[idx][0]), int(landmarks[idx][1])\n",
    "\n",
    "                    # Determine point size based on attention/importance - MUCH LARGER\n",
    "                    base_radius = 6  # Increased from 3\n",
    "                    if attention_weights is not None and idx < len(attention_weights):\n",
    "                        # Scale radius based on attention weight\n",
    "                        attention_val = float(attention_weights[idx])\n",
    "                        radius = max(4, int(6 + attention_val * 8))  # Larger range\n",
    "                    else:\n",
    "                        radius = base_radius\n",
    "\n",
    "                    # Draw outer circle (border) for better visibility\n",
    "                    cv2.circle(overlay, (x, y), radius + 2, (255, 255, 255), 2)  # White border\n",
    "                    # Draw main point\n",
    "                    cv2.circle(overlay, (x, y), radius, color, -1)\n",
    "\n",
    "        # Connect landmarks within regions with thicker lines\n",
    "        self.draw_connections(overlay, landmarks)\n",
    "\n",
    "        # Blend overlay with original image\n",
    "        alpha = 0.5  # Increased opacity for better visibility\n",
    "        image = cv2.addWeighted(overlay, alpha, image, 1 - alpha, 0)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def draw_connections(self, image, landmarks):\n",
    "        \"\"\"Draw connections between landmarks with thicker lines\"\"\"\n",
    "        if landmarks is None or len(landmarks) < 68:\n",
    "            return\n",
    "\n",
    "        line_thickness = 1  # Increased from 1\n",
    "\n",
    "        # Jaw line connections\n",
    "        jaw_indices = self.regions['jaw']\n",
    "        for i in range(len(jaw_indices) - 1):\n",
    "            if jaw_indices[i] < len(landmarks) and jaw_indices[i+1] < len(landmarks):\n",
    "                pt1 = tuple(map(int, landmarks[jaw_indices[i]]))\n",
    "                pt2 = tuple(map(int, landmarks[jaw_indices[i+1]]))\n",
    "                cv2.line(image, pt1, pt2, self.region_colors['jaw'], line_thickness)\n",
    "\n",
    "        # Eye connections (simplified)\n",
    "        eye_regions = [(36, 42), (42, 48)]  # Right and left eye ranges\n",
    "        for start, end in eye_regions:\n",
    "            for i in range(start, end - 1):\n",
    "                if i < len(landmarks) and i + 1 < len(landmarks):\n",
    "                    pt1 = tuple(map(int, landmarks[i]))\n",
    "                    pt2 = tuple(map(int, landmarks[i + 1]))\n",
    "                    cv2.line(image, pt1, pt2, self.region_colors['eyes'], line_thickness)\n",
    "\n",
    "        # Mouth connections\n",
    "        mouth_indices = self.regions['mouth']\n",
    "        for i in range(len(mouth_indices) - 1):\n",
    "            if mouth_indices[i] < len(landmarks) and mouth_indices[i+1] < len(landmarks):\n",
    "                pt1 = tuple(map(int, landmarks[mouth_indices[i]]))\n",
    "                pt2 = tuple(map(int, landmarks[mouth_indices[i+1]]))\n",
    "                cv2.line(image, pt1, pt2, self.region_colors['mouth'], line_thickness)\n",
    "\n",
    "    def draw_info_panel(self, image, emotions, attention_weights, feature_count, buffer_status=\"\"):\n",
    "        \"\"\"Draw information panel with predictions and feature stats - FIXED VERSION\"\"\"\n",
    "        h, w = image.shape[:2]\n",
    "        panel_width = 400  # Increased width for better visibility\n",
    "        panel_height = h\n",
    "\n",
    "        # Create info panel with darker background\n",
    "        panel = np.zeros((panel_height, panel_width, 3), dtype=np.uint8)\n",
    "        panel.fill(20)  # Very dark background\n",
    "\n",
    "        # Add border to panel\n",
    "        cv2.rectangle(panel, (0, 0), (panel_width-1, panel_height-1), (100, 100, 100), 2)\n",
    "\n",
    "        y_offset = 30\n",
    "        line_height = 25\n",
    "        font_scale = 0.6\n",
    "        font_thickness = 2\n",
    "\n",
    "        # Title with background\n",
    "        title_bg = np.zeros((40, panel_width, 3), dtype=np.uint8)\n",
    "        title_bg.fill(60)\n",
    "        panel[0:40, :] = title_bg\n",
    "        cv2.putText(panel, \"SWING TRANSFORMER ANALYSIS\", (10, 25),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        y_offset = 60\n",
    "\n",
    "        # Buffer status\n",
    "        status_color = (0, 255, 255) if \"Ready\" in buffer_status else (255, 255, 0)\n",
    "        cv2.putText(panel, f\"Status: {buffer_status}\", (10, y_offset),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, status_color, 1)\n",
    "        y_offset += line_height\n",
    "\n",
    "        # Feature count\n",
    "        cv2.putText(panel, f\"Active Features: {feature_count}/68\", (10, y_offset),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "        y_offset += line_height + 10\n",
    "\n",
    "        # Section separator\n",
    "        cv2.line(panel, (10, y_offset), (panel_width-10, y_offset), (100, 100, 100), 1)\n",
    "        y_offset += 20\n",
    "\n",
    "        # Emotion predictions section\n",
    "        cv2.putText(panel, \"EMOTION PREDICTIONS:\", (10, y_offset),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        y_offset += 30\n",
    "\n",
    "        if emotions is not None:\n",
    "            # Get emotion probabilities\n",
    "            if isinstance(emotions, torch.Tensor):\n",
    "                emotion_probs = F.softmax(emotions, dim=1)[0].detach().numpy()\n",
    "            else:\n",
    "                emotion_probs = F.softmax(torch.tensor(emotions), dim=0).numpy()\n",
    "\n",
    "            # Sort emotions by probability for better display\n",
    "            emotion_data = list(zip(self.emotion_labels, emotion_probs))\n",
    "            emotion_data.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            for i, (label, prob) in enumerate(emotion_data):\n",
    "                # Color coding based on probability\n",
    "                if prob > 0.4:\n",
    "                    color = (0, 255, 0)  # High confidence - Green\n",
    "                elif prob > 0.2:\n",
    "                    color = (0, 255, 255)  # Medium confidence - Yellow\n",
    "                else:\n",
    "                    color = (128, 128, 128)  # Low confidence - Gray\n",
    "\n",
    "                # Emotion label and value\n",
    "                cv2.putText(panel, f\"{label}: {prob:.3f}\", (15, y_offset),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "                # Draw probability bar\n",
    "                bar_width = int(prob * 250)  # Increased bar width\n",
    "                bar_height = 8\n",
    "                # Background bar\n",
    "                cv2.rectangle(panel, (15, y_offset + 8), (265, y_offset + 16), (50, 50, 50), -1)\n",
    "                # Probability bar\n",
    "                if bar_width > 0:\n",
    "                    cv2.rectangle(panel, (15, y_offset + 8), (15 + bar_width, y_offset + 16), color, -1)\n",
    "\n",
    "                y_offset += 28\n",
    "\n",
    "        else:\n",
    "            cv2.putText(panel, \"No emotions detected\", (15, y_offset),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 128, 128), 1)\n",
    "            y_offset += line_height\n",
    "\n",
    "        # Section separator\n",
    "        y_offset += 10\n",
    "        cv2.line(panel, (10, y_offset), (panel_width-10, y_offset), (100, 100, 100), 1)\n",
    "        y_offset += 20\n",
    "\n",
    "        # Attention visualization section\n",
    "        cv2.putText(panel, \"FEATURE ATTENTION:\", (10, y_offset),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        y_offset += 30\n",
    "\n",
    "        # Region attention summary\n",
    "        if attention_weights is not None:\n",
    "            region_attention = {}\n",
    "            att_weights = attention_weights if isinstance(attention_weights, np.ndarray) else attention_weights.numpy()\n",
    "            \n",
    "            for region_name, indices in self.regions.items():\n",
    "                region_att = np.mean([att_weights[i] if i < len(att_weights) else 0.1 for i in indices])\n",
    "                region_attention[region_name] = region_att\n",
    "\n",
    "            # Sort regions by attention\n",
    "            sorted_regions = sorted(region_attention.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            for region_name, att_val in sorted_regions:\n",
    "                color = self.region_colors[region_name]\n",
    "                cv2.putText(panel, f\"{region_name.upper()}: {att_val:.3f}\", (15, y_offset),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "                # Draw attention bar\n",
    "                bar_width = int(att_val * 200)\n",
    "                # Background bar\n",
    "                cv2.rectangle(panel, (15, y_offset + 8), (215, y_offset + 16), (50, 50, 50), -1)\n",
    "                # Attention bar\n",
    "                if bar_width > 0:\n",
    "                    cv2.rectangle(panel, (15, y_offset + 8), (15 + bar_width, y_offset + 16), color, -1)\n",
    "\n",
    "                y_offset += 28\n",
    "\n",
    "        else:\n",
    "            cv2.putText(panel, \"Computing attention...\", (15, y_offset),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (128, 128, 128), 1)\n",
    "\n",
    "        # Combine panel with main image\n",
    "        combined = np.hstack([image, panel])\n",
    "        return combined\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame - ENHANCED VERSION\"\"\"\n",
    "        # Extract landmarks\n",
    "        landmarks = self.landmark_processor.extract_landmarks(frame)\n",
    "\n",
    "        if landmarks is not None:\n",
    "            # Normalize landmarks\n",
    "            normalized_landmarks = self.normalize_landmarks(landmarks)\n",
    "\n",
    "            # Add to buffer\n",
    "            if normalized_landmarks is not None:\n",
    "                self.landmark_buffer.append(normalized_landmarks)\n",
    "\n",
    "            # Process with transformer\n",
    "            emotions = None\n",
    "            attention_weights = None\n",
    "            buffer_status = f\"Buffer: {len(self.landmark_buffer)}/{self.sequence_length}\"\n",
    "\n",
    "            if len(self.landmark_buffer) >= 5:  # Start predictions with fewer frames\n",
    "                # Prepare input tensor\n",
    "                current_buffer = list(self.landmark_buffer)\n",
    "                \n",
    "                if len(current_buffer) >= self.sequence_length:\n",
    "                    # Full sequence analysis\n",
    "                    sequence = np.array(current_buffer)\n",
    "                    sequence_flat = sequence.reshape(sequence.shape[0], -1)\n",
    "                    sequence_tensor = torch.FloatTensor(sequence_flat).unsqueeze(0)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        emotions, attention_weights = self.model(sequence_tensor, single_frame_mode=False)\n",
    "                        attention_weights = attention_weights[0].numpy()\n",
    "                    \n",
    "                    buffer_status = \"Full Sequence Analysis\"\n",
    "                else:\n",
    "                    # Single frame analysis for early predictions\n",
    "                    current_landmarks = normalized_landmarks.flatten()\n",
    "                    landmarks_tensor = torch.FloatTensor(current_landmarks).unsqueeze(0)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        emotions, attention_weights = self.model(landmarks_tensor, single_frame_mode=True)\n",
    "                        if isinstance(attention_weights, torch.Tensor):\n",
    "                            attention_weights = attention_weights[0].numpy()\n",
    "                    \n",
    "                    buffer_status = f\"Single Frame Analysis ({len(current_buffer)} frames)\"\n",
    "\n",
    "            else:\n",
    "                buffer_status = f\"Collecting frames... ({len(self.landmark_buffer)}/5)\"\n",
    "\n",
    "            # Draw landmarks and info\n",
    "            frame_with_landmarks = self.draw_landmarks_with_features(\n",
    "                frame, landmarks, attention_weights\n",
    "            )\n",
    "\n",
    "            # Add info panel\n",
    "            feature_count = len(landmarks) if landmarks is not None else 0\n",
    "            final_frame = self.draw_info_panel(\n",
    "                frame_with_landmarks, emotions, attention_weights, feature_count, buffer_status\n",
    "            )\n",
    "\n",
    "            return final_frame\n",
    "\n",
    "        else:\n",
    "            # No face detected\n",
    "            no_face_panel = self.draw_info_panel(\n",
    "                frame, None, None, 0, \"No face detected\"\n",
    "            )\n",
    "            return no_face_panel\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc980aa8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Colab-specific helper functions\n",
    "def process_uploaded_video(uploaded_file_path, output_name=\"processed_video.mp4\", max_frames=300):\n",
    "    \"\"\"Process uploaded video in Colab\"\"\"\n",
    "    output_path = f\"/content/{output_name}\"\n",
    "    return main(uploaded_file_path, output_path, max_frames)\n",
    "\n",
    "def process_sample_frames(video_path, num_frames=50):\n",
    "    \"\"\"Process only first N frames for quick testing\"\"\"\n",
    "    print(f\"Processing first {num_frames} frames for quick preview...\")\n",
    "    return main(video_path, None, num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560ca4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main(video_path=None, output_path=None, max_frames=None):\n",
    "    \"\"\"Main function for Colab video processing\"\"\"\n",
    "    print(\"Initializing Swing Transformer Video Processor...\")\n",
    "    processor = RealTimeVideoProcessor(sequence_length=30)\n",
    "\n",
    "    # Initialize video capture\n",
    "    if video_path is None:\n",
    "        print(\"Please provide a video file path\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file: {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    print(f\"Video properties: {width}x{height}, {fps} FPS, {total_frames} frames\")\n",
    "\n",
    "    # Setup output video writer if output path is provided\n",
    "    out = None\n",
    "    if output_path:\n",
    "        # Output will be wider due to info panel\n",
    "        output_width = width + 400  # Updated panel width\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (output_width, height))\n",
    "        print(f\"Output will be saved to: {output_path}\")\n",
    "\n",
    "    # Process frames\n",
    "    frame_count = 0\n",
    "    processed_frames = []\n",
    "\n",
    "    print(\"Processing video frames...\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Finished processing all frames\")\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # Process frame\n",
    "        processed_frame = processor.process_frame(frame)\n",
    "\n",
    "        # Add frame counter\n",
    "        cv2.putText(processed_frame, f\"Frame: {frame_count}/{total_frames}\", (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Save to output video\n",
    "        if out is not None:\n",
    "            out.write(processed_frame)\n",
    "\n",
    "        # Store processed frames for display\n",
    "        # if len(processed_frames) < 16:\n",
    "        #     processed_frames.append(processed_frame.copy())\n",
    "        processed_frames.append(processed_frame.copy())\n",
    "        if len(processed_frames) > 16:\n",
    "            processed_frames.pop(0)\n",
    "        # Progress update\n",
    "        if frame_count % 30 == 0:\n",
    "            progress = (frame_count / total_frames) * 100\n",
    "            print(f\"Progress: {progress:.1f}% ({frame_count}/{total_frames}) - Emotions: {'Active' if len(processor.landmark_buffer) >= 5 else 'Buffering'}\")\n",
    "\n",
    "        # Stop if max_frames limit reached\n",
    "        if max_frames and frame_count >= max_frames:\n",
    "            print(f\"Reached maximum frame limit: {max_frames}\")\n",
    "            break\n",
    "\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "        print(f\"Output video saved successfully: {output_path}\")\n",
    "\n",
    "    print(f\"Video processing completed. Processed {frame_count} frames.\")\n",
    "\n",
    "    # Display sample frames in Colab\n",
    "    if processed_frames:\n",
    "        print(\"\\nDisplaying sample processed frames:\")\n",
    "        from IPython.display import display, Image\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # Show 16 processed frames in 4x4 grid\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(24, 18))  # Even larger for panel visibility\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, frame in enumerate(processed_frames[:16]):\n",
    "            if i < len(axes):\n",
    "                # Convert BGR to RGB for matplotlib\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                axes[i].imshow(rgb_frame)\n",
    "                axes[i].set_title(f'Frame {i+1}', fontsize=12)\n",
    "                axes[i].axis('off')\n",
    "\n",
    "        # Hide any unused subplots\n",
    "        for i in range(len(processed_frames), len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return processed_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e97fe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage for Colab:\n",
    "\n",
    "    # For uploaded video file:\n",
    "    processed_frames = process_uploaded_video(\"/content/video2.mp4\")\n",
    "\n",
    "    # For quick testing with sample frames:\n",
    "    # processed_frames = process_sample_frames(\"/content/your_video.mp4\", num_frames=100)\n",
    "\n",
    "    # For full processing with output:\n",
    "    # processed_frames = main(\"/content/input_video.mp4\", \"/content/output_video.mp4\")\n",
    "\n",
    "    print(\"To use this code in Colab:\")\n",
    "    print(\"1. Upload your video file\")\n",
    "    print(\"2. Call: process_uploaded_video('/content/your_video_file.mp4')\")\n",
    "    print(\"3. Or for quick preview: process_sample_frames('/content/your_video_file.mp4', 50)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
